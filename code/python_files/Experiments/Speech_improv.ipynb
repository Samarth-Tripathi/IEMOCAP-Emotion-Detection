{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import wave\n",
    "import copy\n",
    "import math\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.layers import LSTM, Input, Flatten, Merge,Bidirectional\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "\n",
    "from features import *\n",
    "from helper import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "code_path = os.path.dirname(os.path.realpath(os.getcwd()))\n",
    "emotions_used = np.array(['ang', 'exc', 'neu', 'sad'])\n",
    "data_path = code_path + \"/../data/sessions/\"\n",
    "sessions = ['Session1', 'Session2', 'Session3', 'Session4', 'Session5']\n",
    "framerate = 16000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(data_path + '/../'+'data_collected.pickle', 'rb') as handle:\n",
    "    data2 = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_features(frames, freq, options):\n",
    "    window_sec = 0.2\n",
    "    window_n = int(freq * window_sec)\n",
    "\n",
    "    st_f = stFeatureExtraction(frames, freq, window_n, window_n / 2)\n",
    "\n",
    "    if st_f.shape[1] > 2:\n",
    "        i0 = 1\n",
    "        i1 = st_f.shape[1] - 1\n",
    "        if i1 - i0 < 1:\n",
    "            i1 = i0 + 1\n",
    "        \n",
    "        deriv_st_f = np.zeros((st_f.shape[0], i1 - i0), dtype=float)\n",
    "        for i in range(i0, i1):\n",
    "            i_left = i - 1\n",
    "            i_right = i + 1\n",
    "            deriv_st_f[:st_f.shape[0], i - i0] = st_f[:, i]\n",
    "        return deriv_st_f\n",
    "    elif st_f.shape[1] == 2:\n",
    "        deriv_st_f = np.zeros((st_f.shape[0], 1), dtype=float)\n",
    "        deriv_st_f[:st_f.shape[0], 0] = st_f[:, 0]\n",
    "        return deriv_st_f\n",
    "    else:\n",
    "        deriv_st_f = np.zeros((st_f.shape[0], 1), dtype=float)\n",
    "        deriv_st_f[:st_f.shape[0], 0] = st_f[:, 0]\n",
    "        return deriv_st_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2659, 100, 34)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_speech = []\n",
    "\n",
    "counter = 0\n",
    "for ses_mod in data2:\n",
    "    if 'impro' not in ses_mod['id']:\n",
    "        continue\n",
    "    x_head = ses_mod['signal']\n",
    "    st_features = calculate_features(x_head, framerate, None)\n",
    "    st_features, _ = pad_sequence_into_array(st_features, maxlen=100)\n",
    "    x_train_speech.append( st_features.T )\n",
    "    counter+=1\n",
    "    if(counter%100==0):\n",
    "        print(counter)\n",
    "    \n",
    "x_train_speech = np.array(x_train_speech)\n",
    "x_train_speech.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_model(optimizer='Adadelta'):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(512, return_sequences=True, input_shape=(100, 34)))\n",
    "    model.add(LSTM(256, return_sequences=False))\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(4))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100, 512)          1120256   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 256)               787456    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 2052      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 2,041,348\n",
      "Trainable params: 2,041,348\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = lstm_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2659, 4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y=[]\n",
    "for ses_mod in data2:\n",
    "    if 'impro' not in ses_mod['id']:\n",
    "        continue\n",
    "    Y.append(ses_mod['emotion'])\n",
    "    \n",
    "Y = label_binarize(Y,emotions_used)\n",
    "\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/models.py:837: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2127 samples, validate on 532 samples\n",
      "Epoch 1/60\n",
      "2127/2127 [==============================] - 27s - loss: 1.3567 - acc: 0.3780 - val_loss: 1.2005 - val_acc: 0.5301\n",
      "Epoch 2/60\n",
      "2127/2127 [==============================] - 8s - loss: 1.3189 - acc: 0.3855 - val_loss: 1.1977 - val_acc: 0.5132\n",
      "Epoch 3/60\n",
      "2127/2127 [==============================] - 8s - loss: 1.3185 - acc: 0.3883 - val_loss: 1.2156 - val_acc: 0.5132\n",
      "Epoch 4/60\n",
      "2127/2127 [==============================] - 8s - loss: 1.3164 - acc: 0.3874 - val_loss: 1.1594 - val_acc: 0.5301\n",
      "Epoch 5/60\n",
      "2127/2127 [==============================] - 8s - loss: 1.3154 - acc: 0.3883 - val_loss: 1.1900 - val_acc: 0.5150\n",
      "Epoch 6/60\n",
      "2127/2127 [==============================] - 8s - loss: 1.3127 - acc: 0.3869 - val_loss: 1.1961 - val_acc: 0.5169\n",
      "Epoch 7/60\n",
      "2127/2127 [==============================] - 8s - loss: 1.3142 - acc: 0.3893 - val_loss: 1.2119 - val_acc: 0.5075\n",
      "Epoch 8/60\n",
      "2127/2127 [==============================] - 8s - loss: 1.3138 - acc: 0.3883 - val_loss: 1.2196 - val_acc: 0.4850\n",
      "Epoch 9/60\n",
      "2127/2127 [==============================] - 8s - loss: 1.3115 - acc: 0.3874 - val_loss: 1.2008 - val_acc: 0.5263\n",
      "Epoch 10/60\n",
      "2127/2127 [==============================] - 8s - loss: 1.3222 - acc: 0.3785 - val_loss: 1.2716 - val_acc: 0.4643\n",
      "Epoch 11/60\n",
      "2127/2127 [==============================] - 8s - loss: 1.3089 - acc: 0.3803 - val_loss: 1.1720 - val_acc: 0.5169\n",
      "Epoch 12/60\n",
      "2127/2127 [==============================] - 8s - loss: 1.3135 - acc: 0.3827 - val_loss: 1.1861 - val_acc: 0.5132\n",
      "Epoch 13/60\n",
      "2127/2127 [==============================] - 8s - loss: 1.2610 - acc: 0.3672 - val_loss: 1.1333 - val_acc: 0.5301\n",
      "Epoch 14/60\n",
      "2127/2127 [==============================] - 8s - loss: 1.2323 - acc: 0.3935 - val_loss: 1.1723 - val_acc: 0.5150\n",
      "Epoch 15/60\n",
      "2127/2127 [==============================] - 8s - loss: 1.2779 - acc: 0.3846 - val_loss: 1.3234 - val_acc: 0.2688\n",
      "Epoch 16/60\n",
      "2127/2127 [==============================] - 8s - loss: 1.2679 - acc: 0.4010 - val_loss: 1.1431 - val_acc: 0.5113\n",
      "Epoch 17/60\n",
      "2127/2127 [==============================] - 8s - loss: 1.1547 - acc: 0.4255 - val_loss: 1.1219 - val_acc: 0.4211\n",
      "Epoch 18/60\n",
      "2127/2127 [==============================] - 8s - loss: 1.1234 - acc: 0.4509 - val_loss: 1.2060 - val_acc: 0.3102\n",
      "Epoch 19/60\n",
      "2127/2127 [==============================] - 8s - loss: 1.1272 - acc: 0.4476 - val_loss: 0.9896 - val_acc: 0.5733\n",
      "Epoch 20/60\n",
      "2127/2127 [==============================] - 8s - loss: 1.1566 - acc: 0.4335 - val_loss: 1.1406 - val_acc: 0.3477\n",
      "Epoch 21/60\n",
      "2127/2127 [==============================] - 8s - loss: 1.1001 - acc: 0.4513 - val_loss: 1.0845 - val_acc: 0.4474\n",
      "Epoch 22/60\n",
      "2127/2127 [==============================] - 8s - loss: 1.0944 - acc: 0.4528 - val_loss: 1.0543 - val_acc: 0.5395\n",
      "Epoch 23/60\n",
      "2127/2127 [==============================] - 8s - loss: 1.1166 - acc: 0.4495 - val_loss: 1.0026 - val_acc: 0.5188\n",
      "Epoch 24/60\n",
      "2127/2127 [==============================] - 8s - loss: 1.1003 - acc: 0.4720 - val_loss: 1.5069 - val_acc: 0.2857\n",
      "Epoch 25/60\n",
      "2127/2127 [==============================] - 8s - loss: 1.1069 - acc: 0.4579 - val_loss: 1.1194 - val_acc: 0.4530\n",
      "Epoch 26/60\n",
      "2127/2127 [==============================] - 8s - loss: 1.0741 - acc: 0.4720 - val_loss: 0.9867 - val_acc: 0.5602\n",
      "Epoch 27/60\n",
      "2127/2127 [==============================] - 8s - loss: 1.0678 - acc: 0.5045 - val_loss: 1.0091 - val_acc: 0.5508\n",
      "Epoch 28/60\n",
      "2127/2127 [==============================] - 8s - loss: 1.0645 - acc: 0.4984 - val_loss: 0.9739 - val_acc: 0.5714\n",
      "Epoch 29/60\n",
      "2127/2127 [==============================] - 8s - loss: 1.0684 - acc: 0.4988 - val_loss: 1.1208 - val_acc: 0.5695\n",
      "Epoch 30/60\n",
      "2127/2127 [==============================] - 8s - loss: 1.0756 - acc: 0.5059 - val_loss: 1.0533 - val_acc: 0.5451\n",
      "Epoch 31/60\n",
      "2127/2127 [==============================] - 8s - loss: 1.0441 - acc: 0.5125 - val_loss: 1.2686 - val_acc: 0.4624\n",
      "Epoch 32/60\n",
      "2127/2127 [==============================] - 8s - loss: 1.0741 - acc: 0.4937 - val_loss: 1.0726 - val_acc: 0.5846\n",
      "Epoch 33/60\n",
      "2127/2127 [==============================] - 8s - loss: 1.0645 - acc: 0.5007 - val_loss: 0.9969 - val_acc: 0.5714\n",
      "Epoch 34/60\n",
      "2127/2127 [==============================] - 8s - loss: 1.0324 - acc: 0.5289 - val_loss: 1.6610 - val_acc: 0.2876\n",
      "Epoch 35/60\n",
      "2127/2127 [==============================] - 8s - loss: 1.0398 - acc: 0.5397 - val_loss: 1.1028 - val_acc: 0.5056\n",
      "Epoch 36/60\n",
      "2127/2127 [==============================] - 8s - loss: 1.0133 - acc: 0.5454 - val_loss: 0.9754 - val_acc: 0.5959\n",
      "Epoch 37/60\n",
      "2127/2127 [==============================] - 8s - loss: 1.0255 - acc: 0.5346 - val_loss: 1.0241 - val_acc: 0.6316\n",
      "Epoch 38/60\n",
      "2127/2127 [==============================] - 8s - loss: 1.0258 - acc: 0.5435 - val_loss: 0.9702 - val_acc: 0.6203\n",
      "Epoch 39/60\n",
      "2127/2127 [==============================] - 8s - loss: 1.0345 - acc: 0.5416 - val_loss: 1.5956 - val_acc: 0.3289\n",
      "Epoch 40/60\n",
      "2127/2127 [==============================] - 8s - loss: 1.0177 - acc: 0.5501 - val_loss: 1.2308 - val_acc: 0.3684\n",
      "Epoch 41/60\n",
      "2127/2127 [==============================] - 8s - loss: 1.0224 - acc: 0.5520 - val_loss: 1.0379 - val_acc: 0.5320\n",
      "Epoch 42/60\n",
      "2127/2127 [==============================] - 8s - loss: 1.0229 - acc: 0.5510 - val_loss: 0.9781 - val_acc: 0.6147\n",
      "Epoch 43/60\n",
      "2127/2127 [==============================] - 8s - loss: 1.0003 - acc: 0.5524 - val_loss: 1.4860 - val_acc: 0.3571\n",
      "Epoch 44/60\n",
      "2127/2127 [==============================] - 8s - loss: 1.0153 - acc: 0.5472 - val_loss: 1.4269 - val_acc: 0.3609\n",
      "Epoch 45/60\n",
      "2127/2127 [==============================] - 8s - loss: 0.9896 - acc: 0.5576 - val_loss: 1.1058 - val_acc: 0.4737\n",
      "Epoch 46/60\n",
      "2127/2127 [==============================] - 8s - loss: 0.9711 - acc: 0.5703 - val_loss: 1.9371 - val_acc: 0.3233\n",
      "Epoch 47/60\n",
      "2127/2127 [==============================] - 8s - loss: 1.0055 - acc: 0.5472 - val_loss: 1.1872 - val_acc: 0.4135\n",
      "Epoch 48/60\n",
      "2127/2127 [==============================] - 8s - loss: 0.9784 - acc: 0.5708 - val_loss: 1.0725 - val_acc: 0.5132\n",
      "Epoch 49/60\n",
      "2127/2127 [==============================] - 8s - loss: 0.9909 - acc: 0.5651 - val_loss: 1.1933 - val_acc: 0.4774\n",
      "Epoch 50/60\n",
      "2127/2127 [==============================] - 8s - loss: 0.9633 - acc: 0.5886 - val_loss: 1.1061 - val_acc: 0.5038\n",
      "Epoch 51/60\n",
      "2127/2127 [==============================] - 8s - loss: 0.9706 - acc: 0.5614 - val_loss: 1.1433 - val_acc: 0.5282\n",
      "Epoch 52/60\n",
      "2127/2127 [==============================] - 8s - loss: 0.9578 - acc: 0.5778 - val_loss: 0.9118 - val_acc: 0.6353\n",
      "Epoch 53/60\n",
      "2127/2127 [==============================] - 8s - loss: 0.9628 - acc: 0.5726 - val_loss: 1.1735 - val_acc: 0.4944\n",
      "Epoch 54/60\n",
      "2127/2127 [==============================] - 8s - loss: 0.9608 - acc: 0.5750 - val_loss: 0.9728 - val_acc: 0.5789\n",
      "Epoch 55/60\n",
      "2127/2127 [==============================] - 8s - loss: 0.9584 - acc: 0.5802 - val_loss: 0.9830 - val_acc: 0.6165\n",
      "Epoch 56/60\n",
      "2127/2127 [==============================] - 8s - loss: 0.9443 - acc: 0.5787 - val_loss: 1.2172 - val_acc: 0.4756\n",
      "Epoch 57/60\n",
      "2127/2127 [==============================] - 8s - loss: 0.9568 - acc: 0.5825 - val_loss: 1.3611 - val_acc: 0.3966\n",
      "Epoch 58/60\n",
      "2127/2127 [==============================] - 8s - loss: 0.9470 - acc: 0.5806 - val_loss: 0.9593 - val_acc: 0.6034\n",
      "Epoch 59/60\n",
      "2127/2127 [==============================] - 8s - loss: 0.9332 - acc: 0.5863 - val_loss: 0.9367 - val_acc: 0.6372\n",
      "Epoch 60/60\n",
      "2127/2127 [==============================] - 8s - loss: 0.9255 - acc: 0.5971 - val_loss: 0.9124 - val_acc: 0.6579\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x_train_speech, Y, \n",
    "                 batch_size=100, nb_epoch=60, verbose=1, shuffle = True, \n",
    "                 validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/models.py:837: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2127 samples, validate on 532 samples\n",
      "Epoch 1/15\n",
      "2127/2127 [==============================] - 8s - loss: 0.9284 - acc: 0.6098 - val_loss: 1.0951 - val_acc: 0.5414\n",
      "Epoch 2/15\n",
      "2127/2127 [==============================] - 8s - loss: 0.9361 - acc: 0.5976 - val_loss: 1.0913 - val_acc: 0.5132\n",
      "Epoch 3/15\n",
      "2127/2127 [==============================] - 8s - loss: 0.9207 - acc: 0.6027 - val_loss: 1.0085 - val_acc: 0.6128\n",
      "Epoch 4/15\n",
      "2127/2127 [==============================] - 8s - loss: 0.9290 - acc: 0.5976 - val_loss: 0.9203 - val_acc: 0.6335\n",
      "Epoch 5/15\n",
      "2127/2127 [==============================] - 8s - loss: 0.9153 - acc: 0.6013 - val_loss: 1.4090 - val_acc: 0.4530\n",
      "Epoch 6/15\n",
      "2127/2127 [==============================] - 8s - loss: 0.9328 - acc: 0.5905 - val_loss: 1.0003 - val_acc: 0.5602\n",
      "Epoch 7/15\n",
      "2127/2127 [==============================] - 8s - loss: 0.8968 - acc: 0.6008 - val_loss: 1.2632 - val_acc: 0.4962\n",
      "Epoch 8/15\n",
      "2127/2127 [==============================] - 8s - loss: 0.9171 - acc: 0.6023 - val_loss: 1.1520 - val_acc: 0.5432\n",
      "Epoch 9/15\n",
      "2127/2127 [==============================] - 8s - loss: 0.9151 - acc: 0.6098 - val_loss: 0.9084 - val_acc: 0.6579\n",
      "Epoch 10/15\n",
      "2127/2127 [==============================] - 8s - loss: 0.8997 - acc: 0.6145 - val_loss: 1.0164 - val_acc: 0.5959\n",
      "Epoch 11/15\n",
      "2127/2127 [==============================] - 8s - loss: 0.9113 - acc: 0.6131 - val_loss: 0.9597 - val_acc: 0.6353\n",
      "Epoch 12/15\n",
      "2127/2127 [==============================] - 8s - loss: 0.9064 - acc: 0.6159 - val_loss: 0.9614 - val_acc: 0.6184\n",
      "Epoch 13/15\n",
      "2127/2127 [==============================] - 8s - loss: 0.8815 - acc: 0.6239 - val_loss: 1.5875 - val_acc: 0.4079\n",
      "Epoch 14/15\n",
      "2127/2127 [==============================] - 8s - loss: 0.8900 - acc: 0.6192 - val_loss: 1.0205 - val_acc: 0.6053\n",
      "Epoch 15/15\n",
      "2127/2127 [==============================] - 8s - loss: 0.8813 - acc: 0.6323 - val_loss: 1.0543 - val_acc: 0.6353\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x_train_speech, Y, \n",
    "                 batch_size=100, nb_epoch=15, verbose=1, shuffle = True, \n",
    "                 validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras import regularizers, constraints, initializers, activations\n",
    "from keras.layers.recurrent import Recurrent, _time_distributed_dense\n",
    "from keras.engine import InputSpec\n",
    "\n",
    "tfPrint = lambda d, T: tf.Print(input_=T, data=[T, tf.shape(T)], message=d)\n",
    "\n",
    "class AttentionDecoder(Recurrent):\n",
    "\n",
    "    def __init__(self, units, output_dim,\n",
    "                 activation='tanh',\n",
    "                 return_probabilities=False,\n",
    "                 name='AttentionDecoder',\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 recurrent_initializer='orthogonal',\n",
    "                 bias_initializer='zeros',\n",
    "                 kernel_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 activity_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Implements an AttentionDecoder that takes in a sequence encoded by an\n",
    "        encoder and outputs the decoded states\n",
    "        :param units: dimension of the hidden state and the attention matrices\n",
    "        :param output_dim: the number of labels in the output space\n",
    "\n",
    "        references:\n",
    "            Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio.\n",
    "            \"Neural machine translation by jointly learning to align and translate.\"\n",
    "            arXiv preprint arXiv:1409.0473 (2014).\n",
    "        \"\"\"\n",
    "        self.units = units\n",
    "        self.output_dim = output_dim\n",
    "        self.return_probabilities = return_probabilities\n",
    "        self.activation = activations.get(activation)\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.recurrent_initializer = initializers.get(recurrent_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "\n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.recurrent_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.recurrent_constraint = constraints.get(kernel_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "\n",
    "        super(AttentionDecoder, self).__init__(**kwargs)\n",
    "        self.name = name\n",
    "        self.return_sequences = True  # must return sequences\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "          See Appendix 2 of Bahdanau 2014, arXiv:1409.0473\n",
    "          for model details that correspond to the matrices here.\n",
    "        \"\"\"\n",
    "\n",
    "        self.batch_size, self.timesteps, self.input_dim = input_shape\n",
    "\n",
    "        if self.stateful:\n",
    "            super(AttentionDecoder, self).reset_states()\n",
    "\n",
    "        self.states = [None, None]  # y, s\n",
    "\n",
    "        \"\"\"\n",
    "            Matrices for creating the context vector\n",
    "        \"\"\"\n",
    "\n",
    "        self.V_a = self.add_weight(shape=(self.units,),\n",
    "                                   name='V_a',\n",
    "                                   initializer=self.kernel_initializer,\n",
    "                                   regularizer=self.kernel_regularizer,\n",
    "                                   constraint=self.kernel_constraint)\n",
    "        self.W_a = self.add_weight(shape=(self.units, self.units),\n",
    "                                   name='W_a',\n",
    "                                   initializer=self.kernel_initializer,\n",
    "                                   regularizer=self.kernel_regularizer,\n",
    "                                   constraint=self.kernel_constraint)\n",
    "        self.U_a = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='U_a',\n",
    "                                   initializer=self.kernel_initializer,\n",
    "                                   regularizer=self.kernel_regularizer,\n",
    "                                   constraint=self.kernel_constraint)\n",
    "        self.b_a = self.add_weight(shape=(self.units,),\n",
    "                                   name='b_a',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "        \"\"\"\n",
    "            Matrices for the r (reset) gate\n",
    "        \"\"\"\n",
    "        self.C_r = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='C_r',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.U_r = self.add_weight(shape=(self.units, self.units),\n",
    "                                   name='U_r',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.W_r = self.add_weight(shape=(self.output_dim, self.units),\n",
    "                                   name='W_r',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.b_r = self.add_weight(shape=(self.units, ),\n",
    "                                   name='b_r',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "\n",
    "        \"\"\"\n",
    "            Matrices for the z (update) gate\n",
    "        \"\"\"\n",
    "        self.C_z = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='C_z',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.U_z = self.add_weight(shape=(self.units, self.units),\n",
    "                                   name='U_z',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.W_z = self.add_weight(shape=(self.output_dim, self.units),\n",
    "                                   name='W_z',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.b_z = self.add_weight(shape=(self.units, ),\n",
    "                                   name='b_z',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "        \"\"\"\n",
    "            Matrices for the proposal\n",
    "        \"\"\"\n",
    "        self.C_p = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='C_p',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.U_p = self.add_weight(shape=(self.units, self.units),\n",
    "                                   name='U_p',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.W_p = self.add_weight(shape=(self.output_dim, self.units),\n",
    "                                   name='W_p',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.b_p = self.add_weight(shape=(self.units, ),\n",
    "                                   name='b_p',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "        \"\"\"\n",
    "            Matrices for making the final prediction vector\n",
    "        \"\"\"\n",
    "        self.C_o = self.add_weight(shape=(self.input_dim, self.output_dim),\n",
    "                                   name='C_o',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.U_o = self.add_weight(shape=(self.units, self.output_dim),\n",
    "                                   name='U_o',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.W_o = self.add_weight(shape=(self.output_dim, self.output_dim),\n",
    "                                   name='W_o',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.b_o = self.add_weight(shape=(self.output_dim, ),\n",
    "                                   name='b_o',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "\n",
    "        # For creating the initial state:\n",
    "        self.W_s = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='W_s',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "\n",
    "        self.input_spec = [\n",
    "            InputSpec(shape=(self.batch_size, self.timesteps, self.input_dim))]\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, x):\n",
    "        # store the whole sequence so we can \"attend\" to it at each timestep\n",
    "        self.x_seq = x\n",
    "\n",
    "        # apply the a dense layer over the time dimension of the sequence\n",
    "        # do it here because it doesn't depend on any previous steps\n",
    "        # thefore we can save computation time:\n",
    "        self._uxpb = _time_distributed_dense(self.x_seq, self.U_a, b=self.b_a,\n",
    "                                             input_dim=self.input_dim,\n",
    "                                             timesteps=self.timesteps,\n",
    "                                             output_dim=self.units)\n",
    "\n",
    "        return super(AttentionDecoder, self).call(x)\n",
    "\n",
    "    def get_initial_state(self, inputs):\n",
    "        # apply the matrix on the first time step to get the initial s0.\n",
    "        s0 = activations.tanh(K.dot(inputs[:, 0], self.W_s))\n",
    "\n",
    "        # from keras.layers.recurrent to initialize a vector of (batchsize,\n",
    "        # output_dim)\n",
    "        y0 = K.zeros_like(inputs)  # (samples, timesteps, input_dims)\n",
    "        y0 = K.sum(y0, axis=(1, 2))  # (samples, )\n",
    "        y0 = K.expand_dims(y0)  # (samples, 1)\n",
    "        y0 = K.tile(y0, [1, self.output_dim])\n",
    "\n",
    "        return [y0, s0]\n",
    "\n",
    "    def step(self, x, states):\n",
    "\n",
    "        ytm, stm = states\n",
    "\n",
    "        # repeat the hidden state to the length of the sequence\n",
    "        _stm = K.repeat(stm, self.timesteps)\n",
    "\n",
    "        # now multiplty the weight matrix with the repeated hidden state\n",
    "        _Wxstm = K.dot(_stm, self.W_a)\n",
    "\n",
    "        # calculate the attention probabilities\n",
    "        # this relates how much other timesteps contributed to this one.\n",
    "        et = K.dot(activations.tanh(_Wxstm + self._uxpb),\n",
    "                   K.expand_dims(self.V_a))\n",
    "        at = K.exp(et)\n",
    "        at_sum = K.sum(at, axis=1)\n",
    "        at_sum_repeated = K.repeat(at_sum, self.timesteps)\n",
    "        at /= at_sum_repeated  # vector of size (batchsize, timesteps, 1)\n",
    "\n",
    "        # calculate the context vector\n",
    "        context = K.squeeze(K.batch_dot(at, self.x_seq, axes=1), axis=1)\n",
    "        # ~~~> calculate new hidden state\n",
    "        # first calculate the \"r\" gate:\n",
    "\n",
    "        rt = activations.sigmoid(\n",
    "            K.dot(ytm, self.W_r)\n",
    "            + K.dot(stm, self.U_r)\n",
    "            + K.dot(context, self.C_r)\n",
    "            + self.b_r)\n",
    "\n",
    "        # now calculate the \"z\" gate\n",
    "        zt = activations.sigmoid(\n",
    "            K.dot(ytm, self.W_z)\n",
    "            + K.dot(stm, self.U_z)\n",
    "            + K.dot(context, self.C_z)\n",
    "            + self.b_z)\n",
    "\n",
    "        # calculate the proposal hidden state:\n",
    "        s_tp = activations.tanh(\n",
    "            K.dot(ytm, self.W_p)\n",
    "            + K.dot((rt * stm), self.U_p)\n",
    "            + K.dot(context, self.C_p)\n",
    "            + self.b_p)\n",
    "\n",
    "        # new hidden state:\n",
    "        st = (1-zt)*stm + zt * s_tp\n",
    "\n",
    "        yt = activations.softmax(\n",
    "            K.dot(ytm, self.W_o)\n",
    "            + K.dot(stm, self.U_o)\n",
    "            + K.dot(context, self.C_o)\n",
    "            + self.b_o)\n",
    "\n",
    "        if self.return_probabilities:\n",
    "            return at, [yt, st]\n",
    "        else:\n",
    "            return yt, [yt, st]\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \"\"\"\n",
    "            For Keras internal compatability checking\n",
    "        \"\"\"\n",
    "        if self.return_probabilities:\n",
    "            return (None, self.timesteps, self.timesteps)\n",
    "        else:\n",
    "            return (None, self.timesteps, self.output_dim)\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"\n",
    "            For rebuilding models on load time.\n",
    "        \"\"\"\n",
    "        config = {\n",
    "            'output_dim': self.output_dim,\n",
    "            'units': self.units,\n",
    "            'return_probabilities': self.return_probabilities\n",
    "        }\n",
    "        base_config = super(AttentionDecoder, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_model(optimizer='Adadelta'):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, return_sequences=True, input_shape=(100, 34)))\n",
    "    model.add(AttentionDecoder(128,128))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(4))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 100, 128)          83456     \n",
      "_________________________________________________________________\n",
      "AttentionDecoder (AttentionD (None, 100, 128)          246528    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 12800)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               6554112   \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4)                 2052      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 6,886,148\n",
      "Trainable params: 6,886,148\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = attention_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/models.py:837: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2127 samples, validate on 532 samples\n",
      "Epoch 1/75\n",
      "2127/2127 [==============================] - 14s - loss: 1.3290 - acc: 0.3808 - val_loss: 1.1877 - val_acc: 0.5301\n",
      "Epoch 2/75\n",
      "2127/2127 [==============================] - 13s - loss: 1.3111 - acc: 0.3846 - val_loss: 1.1691 - val_acc: 0.5301\n",
      "Epoch 3/75\n",
      "2127/2127 [==============================] - 13s - loss: 1.2971 - acc: 0.3973 - val_loss: 1.2696 - val_acc: 0.3102\n",
      "Epoch 4/75\n",
      "2127/2127 [==============================] - 13s - loss: 1.2992 - acc: 0.3832 - val_loss: 1.2033 - val_acc: 0.5301\n",
      "Epoch 5/75\n",
      "2127/2127 [==============================] - 13s - loss: 1.2706 - acc: 0.4104 - val_loss: 1.1750 - val_acc: 0.5301\n",
      "Epoch 6/75\n",
      "2127/2127 [==============================] - 13s - loss: 1.2789 - acc: 0.4006 - val_loss: 1.1752 - val_acc: 0.5301\n",
      "Epoch 7/75\n",
      "2127/2127 [==============================] - 13s - loss: 1.2548 - acc: 0.4015 - val_loss: 1.1022 - val_acc: 0.5620\n",
      "Epoch 8/75\n",
      "2127/2127 [==============================] - 13s - loss: 1.1860 - acc: 0.4269 - val_loss: 1.2430 - val_acc: 0.5301\n",
      "Epoch 9/75\n",
      "2127/2127 [==============================] - 13s - loss: 1.1873 - acc: 0.4419 - val_loss: 1.0535 - val_acc: 0.5414\n",
      "Epoch 10/75\n",
      "2127/2127 [==============================] - 13s - loss: 1.1039 - acc: 0.4843 - val_loss: 0.9777 - val_acc: 0.5752\n",
      "Epoch 11/75\n",
      "2127/2127 [==============================] - 13s - loss: 1.1347 - acc: 0.4748 - val_loss: 1.1936 - val_acc: 0.3853\n",
      "Epoch 12/75\n",
      "2127/2127 [==============================] - 13s - loss: 1.1105 - acc: 0.4786 - val_loss: 1.0698 - val_acc: 0.5019\n",
      "Epoch 13/75\n",
      "2127/2127 [==============================] - 13s - loss: 1.1078 - acc: 0.5049 - val_loss: 0.9810 - val_acc: 0.6241\n",
      "Epoch 14/75\n",
      "2127/2127 [==============================] - 13s - loss: 1.0899 - acc: 0.4866 - val_loss: 1.0809 - val_acc: 0.4756\n",
      "Epoch 15/75\n",
      "2127/2127 [==============================] - 13s - loss: 1.0343 - acc: 0.5336 - val_loss: 1.1154 - val_acc: 0.4511\n",
      "Epoch 16/75\n",
      "2127/2127 [==============================] - 13s - loss: 1.0896 - acc: 0.5007 - val_loss: 1.0125 - val_acc: 0.5714\n",
      "Epoch 17/75\n",
      "2127/2127 [==============================] - 13s - loss: 1.0392 - acc: 0.5416 - val_loss: 0.9731 - val_acc: 0.5959\n",
      "Epoch 18/75\n",
      "2127/2127 [==============================] - 13s - loss: 1.0405 - acc: 0.5270 - val_loss: 1.0367 - val_acc: 0.5996\n",
      "Epoch 19/75\n",
      "2127/2127 [==============================] - 13s - loss: 1.0377 - acc: 0.5308 - val_loss: 1.0186 - val_acc: 0.5451\n",
      "Epoch 20/75\n",
      "2127/2127 [==============================] - 13s - loss: 1.0458 - acc: 0.5242 - val_loss: 0.9478 - val_acc: 0.6109\n",
      "Epoch 21/75\n",
      "2127/2127 [==============================] - 13s - loss: 1.0294 - acc: 0.5299 - val_loss: 1.1459 - val_acc: 0.4323\n",
      "Epoch 22/75\n",
      "2127/2127 [==============================] - 13s - loss: 1.0095 - acc: 0.5505 - val_loss: 1.0134 - val_acc: 0.5451\n",
      "Epoch 23/75\n",
      "2127/2127 [==============================] - 13s - loss: 1.0227 - acc: 0.5393 - val_loss: 1.0528 - val_acc: 0.5094\n",
      "Epoch 24/75\n",
      "2127/2127 [==============================] - 13s - loss: 1.0034 - acc: 0.5585 - val_loss: 0.9549 - val_acc: 0.6579\n",
      "Epoch 25/75\n",
      "2127/2127 [==============================] - 13s - loss: 0.9784 - acc: 0.5656 - val_loss: 0.9119 - val_acc: 0.6579\n",
      "Epoch 26/75\n",
      "2127/2127 [==============================] - 13s - loss: 0.9941 - acc: 0.5510 - val_loss: 0.9475 - val_acc: 0.5940\n",
      "Epoch 27/75\n",
      "2127/2127 [==============================] - 13s - loss: 0.9765 - acc: 0.5708 - val_loss: 1.1843 - val_acc: 0.4492\n",
      "Epoch 28/75\n",
      "2127/2127 [==============================] - 13s - loss: 0.9906 - acc: 0.5571 - val_loss: 0.9241 - val_acc: 0.5959\n",
      "Epoch 29/75\n",
      "2127/2127 [==============================] - 13s - loss: 0.9701 - acc: 0.5665 - val_loss: 0.9923 - val_acc: 0.5883\n",
      "Epoch 30/75\n",
      "2127/2127 [==============================] - 13s - loss: 0.9554 - acc: 0.5722 - val_loss: 0.9374 - val_acc: 0.6297\n",
      "Epoch 31/75\n",
      "2127/2127 [==============================] - 13s - loss: 0.9620 - acc: 0.5689 - val_loss: 1.0615 - val_acc: 0.5113\n",
      "Epoch 32/75\n",
      "2127/2127 [==============================] - 13s - loss: 0.9455 - acc: 0.5839 - val_loss: 0.9211 - val_acc: 0.6147\n",
      "Epoch 33/75\n",
      "2127/2127 [==============================] - 13s - loss: 0.9539 - acc: 0.5759 - val_loss: 0.9222 - val_acc: 0.6335\n",
      "Epoch 34/75\n",
      "2127/2127 [==============================] - 13s - loss: 0.9249 - acc: 0.5886 - val_loss: 1.0073 - val_acc: 0.5996\n",
      "Epoch 35/75\n",
      "2127/2127 [==============================] - 13s - loss: 0.9517 - acc: 0.5773 - val_loss: 1.2086 - val_acc: 0.4248\n",
      "Epoch 36/75\n",
      "2127/2127 [==============================] - 13s - loss: 0.9188 - acc: 0.5994 - val_loss: 1.2700 - val_acc: 0.4192\n",
      "Epoch 37/75\n",
      "2127/2127 [==============================] - 13s - loss: 0.9356 - acc: 0.5820 - val_loss: 0.9555 - val_acc: 0.6259\n",
      "Epoch 38/75\n",
      "2127/2127 [==============================] - 13s - loss: 0.8991 - acc: 0.6008 - val_loss: 0.9879 - val_acc: 0.5583\n",
      "Epoch 39/75\n",
      "2127/2127 [==============================] - 13s - loss: 0.9067 - acc: 0.5999 - val_loss: 1.0545 - val_acc: 0.5451\n",
      "Epoch 40/75\n",
      "2127/2127 [==============================] - 13s - loss: 0.8888 - acc: 0.6182 - val_loss: 1.0062 - val_acc: 0.5639\n",
      "Epoch 41/75\n",
      "2127/2127 [==============================] - 13s - loss: 0.8963 - acc: 0.6093 - val_loss: 0.8956 - val_acc: 0.6391\n",
      "Epoch 42/75\n",
      "2127/2127 [==============================] - 13s - loss: 0.8998 - acc: 0.6027 - val_loss: 1.0446 - val_acc: 0.5451\n",
      "Epoch 43/75\n",
      "2127/2127 [==============================] - 13s - loss: 0.8942 - acc: 0.6117 - val_loss: 0.9157 - val_acc: 0.6335\n",
      "Epoch 44/75\n",
      "2127/2127 [==============================] - 13s - loss: 0.8847 - acc: 0.6121 - val_loss: 0.8815 - val_acc: 0.6560\n",
      "Epoch 45/75\n",
      "2127/2127 [==============================] - 13s - loss: 0.8742 - acc: 0.6197 - val_loss: 0.8662 - val_acc: 0.6466\n",
      "Epoch 46/75\n",
      "2127/2127 [==============================] - 13s - loss: 0.8812 - acc: 0.6107 - val_loss: 0.8983 - val_acc: 0.6466\n",
      "Epoch 47/75\n",
      "2127/2127 [==============================] - 13s - loss: 0.8617 - acc: 0.6201 - val_loss: 0.8470 - val_acc: 0.6560\n",
      "Epoch 48/75\n",
      "2127/2127 [==============================] - 13s - loss: 0.8653 - acc: 0.6286 - val_loss: 1.0371 - val_acc: 0.5639\n",
      "Epoch 49/75\n",
      "2127/2127 [==============================] - 13s - loss: 0.8552 - acc: 0.6187 - val_loss: 0.8780 - val_acc: 0.6316\n",
      "Epoch 50/75\n",
      "2127/2127 [==============================] - 13s - loss: 0.8474 - acc: 0.6211 - val_loss: 1.0842 - val_acc: 0.5526\n",
      "Epoch 51/75\n",
      "2127/2127 [==============================] - 13s - loss: 0.8296 - acc: 0.6507 - val_loss: 0.9169 - val_acc: 0.5959\n",
      "Epoch 52/75\n",
      "2127/2127 [==============================] - 13s - loss: 0.8404 - acc: 0.6422 - val_loss: 0.8718 - val_acc: 0.6391\n",
      "Epoch 53/75\n",
      "2127/2127 [==============================] - 13s - loss: 0.8244 - acc: 0.6450 - val_loss: 1.0631 - val_acc: 0.5150\n",
      "Epoch 54/75\n",
      "2127/2127 [==============================] - 13s - loss: 0.8288 - acc: 0.6403 - val_loss: 0.9255 - val_acc: 0.5921\n",
      "Epoch 55/75\n",
      "2127/2127 [==============================] - 13s - loss: 0.8414 - acc: 0.6338 - val_loss: 0.8807 - val_acc: 0.6466\n",
      "Epoch 56/75\n",
      "2127/2127 [==============================] - 13s - loss: 0.8357 - acc: 0.6450 - val_loss: 1.1115 - val_acc: 0.4981\n",
      "Epoch 57/75\n",
      "2127/2127 [==============================] - 13s - loss: 0.8027 - acc: 0.6554 - val_loss: 0.9474 - val_acc: 0.5771\n",
      "Epoch 58/75\n",
      "2127/2127 [==============================] - 13s - loss: 0.7982 - acc: 0.6756 - val_loss: 0.8551 - val_acc: 0.6391\n",
      "Epoch 59/75\n",
      "2127/2127 [==============================] - 13s - loss: 0.8023 - acc: 0.6530 - val_loss: 0.9023 - val_acc: 0.5996\n",
      "Epoch 60/75\n",
      "2127/2127 [==============================] - 13s - loss: 0.8002 - acc: 0.6530 - val_loss: 1.0024 - val_acc: 0.5564\n",
      "Epoch 61/75\n",
      "2127/2127 [==============================] - 13s - loss: 0.7835 - acc: 0.6620 - val_loss: 0.9619 - val_acc: 0.5752\n",
      "Epoch 62/75\n",
      "2127/2127 [==============================] - 13s - loss: 0.7885 - acc: 0.6634 - val_loss: 1.0294 - val_acc: 0.5733\n",
      "Epoch 63/75\n",
      "2127/2127 [==============================] - 13s - loss: 0.7772 - acc: 0.6718 - val_loss: 0.9887 - val_acc: 0.5432\n",
      "Epoch 64/75\n",
      "2127/2127 [==============================] - 13s - loss: 0.7802 - acc: 0.6714 - val_loss: 1.0469 - val_acc: 0.5432\n",
      "Epoch 65/75\n",
      "2127/2127 [==============================] - 13s - loss: 0.7597 - acc: 0.6700 - val_loss: 1.0401 - val_acc: 0.5771\n",
      "Epoch 66/75\n",
      "2127/2127 [==============================] - 13s - loss: 0.7548 - acc: 0.6765 - val_loss: 0.8749 - val_acc: 0.6598\n",
      "Epoch 67/75\n",
      "2127/2127 [==============================] - 13s - loss: 0.7445 - acc: 0.6874 - val_loss: 1.1285 - val_acc: 0.5056\n",
      "Epoch 68/75\n",
      "2127/2127 [==============================] - 13s - loss: 0.7556 - acc: 0.6789 - val_loss: 1.0604 - val_acc: 0.5564\n",
      "Epoch 69/75\n",
      "2127/2127 [==============================] - 13s - loss: 0.7427 - acc: 0.6888 - val_loss: 0.9106 - val_acc: 0.6128\n",
      "Epoch 70/75\n",
      "2127/2127 [==============================] - 13s - loss: 0.7329 - acc: 0.6902 - val_loss: 1.0037 - val_acc: 0.5677\n",
      "Epoch 71/75\n",
      "2127/2127 [==============================] - 13s - loss: 0.7451 - acc: 0.6822 - val_loss: 0.9594 - val_acc: 0.5771\n",
      "Epoch 72/75\n",
      "2127/2127 [==============================] - 13s - loss: 0.7413 - acc: 0.6906 - val_loss: 0.9110 - val_acc: 0.6316\n",
      "Epoch 73/75\n",
      "2127/2127 [==============================] - 13s - loss: 0.7178 - acc: 0.6958 - val_loss: 0.9068 - val_acc: 0.6297\n",
      "Epoch 74/75\n",
      "2127/2127 [==============================] - 13s - loss: 0.7190 - acc: 0.6986 - val_loss: 1.0196 - val_acc: 0.5564\n",
      "Epoch 75/75\n",
      "2127/2127 [==============================] - 13s - loss: 0.7230 - acc: 0.6977 - val_loss: 0.8288 - val_acc: 0.6654\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x_train_speech, Y, \n",
    "                 batch_size=100, nb_epoch=75, verbose=1, shuffle = True, \n",
    "                 validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2034"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter = 0\n",
    "for ses_mod in data2:\n",
    "    if 'impro' not in ses_mod['id']:\n",
    "        continue\n",
    "    if (ses_mod['id'][:5]==\"Ses05\"):\n",
    "        break\n",
    "    counter+=1\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_sp = x_train_speech[:2034]\n",
    "xtest_sp = x_train_speech[2034:]\n",
    "ytrain_sp = Y[:2034]\n",
    "ytest_sp = Y[2034:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_3 (Bidirection (None, 100, 256)          166912    \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 100, 256)          689664    \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 25600)             0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 512)               13107712  \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 4)                 2052      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 13,966,340\n",
      "Trainable params: 13,966,340\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def attention_model(optimizer='Adadelta'):\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(128, return_sequences=True), input_shape=(100, 34)))\n",
    "    model.add(Bidirectional(AttentionDecoder(128,128)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(4))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "model = attention_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/models.py:837: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2034 samples, validate on 625 samples\n",
      "Epoch 1/100\n",
      "2034/2034 [==============================] - 29s - loss: 1.3196 - acc: 0.3992 - val_loss: 1.2176 - val_acc: 0.4592\n",
      "Epoch 2/100\n",
      "2034/2034 [==============================] - 27s - loss: 1.3013 - acc: 0.3963 - val_loss: 1.2180 - val_acc: 0.4592\n",
      "Epoch 3/100\n",
      "2034/2034 [==============================] - 27s - loss: 1.2982 - acc: 0.3982 - val_loss: 1.2202 - val_acc: 0.4464\n",
      "Epoch 4/100\n",
      "2034/2034 [==============================] - 27s - loss: 1.2759 - acc: 0.4115 - val_loss: 1.1676 - val_acc: 0.4832\n",
      "Epoch 5/100\n",
      "2034/2034 [==============================] - 27s - loss: 1.2546 - acc: 0.4204 - val_loss: 1.2124 - val_acc: 0.4592\n",
      "Epoch 6/100\n",
      "2034/2034 [==============================] - 27s - loss: 1.2000 - acc: 0.4336 - val_loss: 1.2109 - val_acc: 0.3456\n",
      "Epoch 7/100\n",
      "2034/2034 [==============================] - 27s - loss: 1.1830 - acc: 0.4223 - val_loss: 1.0992 - val_acc: 0.4848\n",
      "Epoch 8/100\n",
      "2034/2034 [==============================] - 27s - loss: 1.1372 - acc: 0.4508 - val_loss: 1.2792 - val_acc: 0.3232\n",
      "Epoch 9/100\n",
      "2034/2034 [==============================] - 27s - loss: 1.1321 - acc: 0.4533 - val_loss: 1.3803 - val_acc: 0.3280\n",
      "Epoch 10/100\n",
      "2034/2034 [==============================] - 27s - loss: 1.0944 - acc: 0.4912 - val_loss: 1.1024 - val_acc: 0.4592\n",
      "Epoch 11/100\n",
      "2034/2034 [==============================] - 27s - loss: 1.0738 - acc: 0.5133 - val_loss: 1.0593 - val_acc: 0.5504\n",
      "Epoch 12/100\n",
      "2034/2034 [==============================] - 27s - loss: 1.0632 - acc: 0.5265 - val_loss: 1.1543 - val_acc: 0.4640\n",
      "Epoch 13/100\n",
      "2034/2034 [==============================] - 27s - loss: 1.0635 - acc: 0.5152 - val_loss: 1.2582 - val_acc: 0.4256\n",
      "Epoch 14/100\n",
      "2034/2034 [==============================] - 27s - loss: 1.0310 - acc: 0.5241 - val_loss: 1.0247 - val_acc: 0.5312\n",
      "Epoch 15/100\n",
      "2034/2034 [==============================] - 27s - loss: 1.0286 - acc: 0.5305 - val_loss: 1.1018 - val_acc: 0.5088\n",
      "Epoch 16/100\n",
      "2034/2034 [==============================] - 27s - loss: 1.0144 - acc: 0.5447 - val_loss: 1.0630 - val_acc: 0.5248\n",
      "Epoch 17/100\n",
      "2034/2034 [==============================] - 27s - loss: 1.0014 - acc: 0.5590 - val_loss: 1.0860 - val_acc: 0.5168\n",
      "Epoch 18/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.9779 - acc: 0.5703 - val_loss: 1.0366 - val_acc: 0.5456\n",
      "Epoch 19/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.9717 - acc: 0.5664 - val_loss: 1.0631 - val_acc: 0.5648\n",
      "Epoch 20/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.9839 - acc: 0.5595 - val_loss: 1.3624 - val_acc: 0.3840\n",
      "Epoch 21/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.9949 - acc: 0.5560 - val_loss: 1.0011 - val_acc: 0.5664\n",
      "Epoch 22/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.9685 - acc: 0.5733 - val_loss: 1.0501 - val_acc: 0.5328\n",
      "Epoch 23/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.9629 - acc: 0.5914 - val_loss: 1.0142 - val_acc: 0.5936\n",
      "Epoch 24/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.9576 - acc: 0.5772 - val_loss: 1.1113 - val_acc: 0.5440\n",
      "Epoch 25/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.9531 - acc: 0.5816 - val_loss: 1.0871 - val_acc: 0.4944\n",
      "Epoch 26/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.9378 - acc: 0.5959 - val_loss: 1.0374 - val_acc: 0.5456\n",
      "Epoch 27/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.9391 - acc: 0.5934 - val_loss: 1.0475 - val_acc: 0.5280\n",
      "Epoch 28/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.9070 - acc: 0.6018 - val_loss: 1.5453 - val_acc: 0.3696\n",
      "Epoch 29/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.9424 - acc: 0.5821 - val_loss: 1.0662 - val_acc: 0.5440\n",
      "Epoch 30/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.9065 - acc: 0.6136 - val_loss: 1.0707 - val_acc: 0.5520\n",
      "Epoch 31/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.9057 - acc: 0.6077 - val_loss: 1.1670 - val_acc: 0.5248\n",
      "Epoch 32/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.8953 - acc: 0.6165 - val_loss: 1.1402 - val_acc: 0.5024\n",
      "Epoch 33/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.8856 - acc: 0.6150 - val_loss: 1.1640 - val_acc: 0.4704\n",
      "Epoch 34/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.8877 - acc: 0.6082 - val_loss: 1.1942 - val_acc: 0.4640\n",
      "Epoch 35/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.8824 - acc: 0.6190 - val_loss: 1.0571 - val_acc: 0.5488\n",
      "Epoch 36/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.8719 - acc: 0.6136 - val_loss: 1.0329 - val_acc: 0.5792\n",
      "Epoch 37/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.8697 - acc: 0.6180 - val_loss: 0.9604 - val_acc: 0.5920\n",
      "Epoch 38/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.8448 - acc: 0.6308 - val_loss: 1.1327 - val_acc: 0.5280\n",
      "Epoch 39/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.8590 - acc: 0.6362 - val_loss: 1.0669 - val_acc: 0.5360\n",
      "Epoch 40/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.8466 - acc: 0.6337 - val_loss: 1.0633 - val_acc: 0.5696\n",
      "Epoch 41/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.8503 - acc: 0.6259 - val_loss: 1.0562 - val_acc: 0.5408\n",
      "Epoch 42/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.8115 - acc: 0.6500 - val_loss: 1.1463 - val_acc: 0.5536\n",
      "Epoch 43/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.8389 - acc: 0.6357 - val_loss: 0.9583 - val_acc: 0.6032\n",
      "Epoch 44/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.8015 - acc: 0.6504 - val_loss: 0.9803 - val_acc: 0.6016\n",
      "Epoch 45/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.7990 - acc: 0.6539 - val_loss: 1.2150 - val_acc: 0.5312\n",
      "Epoch 46/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.7954 - acc: 0.6637 - val_loss: 0.9519 - val_acc: 0.6176\n",
      "Epoch 47/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.7796 - acc: 0.6573 - val_loss: 1.0017 - val_acc: 0.6080\n",
      "Epoch 48/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.7840 - acc: 0.6622 - val_loss: 1.0821 - val_acc: 0.5776\n",
      "Epoch 49/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.7767 - acc: 0.6735 - val_loss: 1.1112 - val_acc: 0.5648\n",
      "Epoch 50/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.7790 - acc: 0.6657 - val_loss: 1.0611 - val_acc: 0.5824\n",
      "Epoch 51/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.7539 - acc: 0.6785 - val_loss: 0.9633 - val_acc: 0.5920\n",
      "Epoch 52/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.7618 - acc: 0.6701 - val_loss: 1.0514 - val_acc: 0.5696\n",
      "Epoch 53/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.7343 - acc: 0.6883 - val_loss: 0.9822 - val_acc: 0.5760\n",
      "Epoch 54/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.7515 - acc: 0.6858 - val_loss: 0.9600 - val_acc: 0.5936\n",
      "Epoch 55/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.7286 - acc: 0.6878 - val_loss: 0.9905 - val_acc: 0.5936\n",
      "Epoch 56/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.7141 - acc: 0.6908 - val_loss: 1.5148 - val_acc: 0.4576\n",
      "Epoch 57/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.7282 - acc: 0.6937 - val_loss: 1.0652 - val_acc: 0.5904\n",
      "Epoch 58/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.7089 - acc: 0.6912 - val_loss: 1.0222 - val_acc: 0.5968\n",
      "Epoch 59/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.7259 - acc: 0.6849 - val_loss: 1.0531 - val_acc: 0.6176\n",
      "Epoch 60/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.7071 - acc: 0.7050 - val_loss: 1.0261 - val_acc: 0.6048\n",
      "Epoch 61/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.6888 - acc: 0.7099 - val_loss: 1.0527 - val_acc: 0.6016\n",
      "Epoch 62/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.6849 - acc: 0.6942 - val_loss: 1.1190 - val_acc: 0.5984\n",
      "Epoch 63/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.6714 - acc: 0.7134 - val_loss: 1.1006 - val_acc: 0.5744\n",
      "Epoch 64/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.6522 - acc: 0.7262 - val_loss: 0.9944 - val_acc: 0.6272\n",
      "Epoch 65/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.6569 - acc: 0.7247 - val_loss: 1.1157 - val_acc: 0.6080\n",
      "Epoch 66/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.6829 - acc: 0.7060 - val_loss: 1.1369 - val_acc: 0.5504\n",
      "Epoch 67/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.6426 - acc: 0.7360 - val_loss: 1.0887 - val_acc: 0.5744\n",
      "Epoch 68/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.6359 - acc: 0.7409 - val_loss: 1.2863 - val_acc: 0.6064\n",
      "Epoch 69/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.6295 - acc: 0.7301 - val_loss: 1.0435 - val_acc: 0.6080\n",
      "Epoch 70/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.6110 - acc: 0.7370 - val_loss: 1.0626 - val_acc: 0.5744\n",
      "Epoch 71/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.6316 - acc: 0.7276 - val_loss: 1.1794 - val_acc: 0.5472\n",
      "Epoch 72/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.6083 - acc: 0.7424 - val_loss: 1.1381 - val_acc: 0.5136\n",
      "Epoch 73/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.6029 - acc: 0.7439 - val_loss: 1.2070 - val_acc: 0.5424\n",
      "Epoch 74/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.6294 - acc: 0.7365 - val_loss: 1.2605 - val_acc: 0.4928\n",
      "Epoch 75/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.6037 - acc: 0.7414 - val_loss: 1.1903 - val_acc: 0.5760\n",
      "Epoch 76/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.5958 - acc: 0.7473 - val_loss: 1.1437 - val_acc: 0.5968\n",
      "Epoch 77/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.5701 - acc: 0.7581 - val_loss: 1.1312 - val_acc: 0.5440\n",
      "Epoch 78/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.5605 - acc: 0.7601 - val_loss: 1.5293 - val_acc: 0.4416\n",
      "Epoch 79/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.5585 - acc: 0.7694 - val_loss: 1.3108 - val_acc: 0.5824\n",
      "Epoch 80/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.5408 - acc: 0.7768 - val_loss: 1.1489 - val_acc: 0.5696\n",
      "Epoch 81/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.5256 - acc: 0.7783 - val_loss: 1.2837 - val_acc: 0.5472\n",
      "Epoch 82/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.5291 - acc: 0.7788 - val_loss: 1.2703 - val_acc: 0.5408\n",
      "Epoch 83/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.5118 - acc: 0.7797 - val_loss: 1.3661 - val_acc: 0.5504\n",
      "Epoch 84/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.5108 - acc: 0.7856 - val_loss: 1.2782 - val_acc: 0.5792\n",
      "Epoch 85/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.5183 - acc: 0.7734 - val_loss: 1.3793 - val_acc: 0.5584\n",
      "Epoch 86/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.4802 - acc: 0.7935 - val_loss: 1.2256 - val_acc: 0.5568\n",
      "Epoch 87/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.4559 - acc: 0.8073 - val_loss: 1.4192 - val_acc: 0.5888\n",
      "Epoch 88/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.4844 - acc: 0.7955 - val_loss: 1.3526 - val_acc: 0.5984\n",
      "Epoch 89/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.4500 - acc: 0.8151 - val_loss: 1.3253 - val_acc: 0.5680\n",
      "Epoch 90/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.4331 - acc: 0.8215 - val_loss: 1.3574 - val_acc: 0.5968\n",
      "Epoch 91/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.4872 - acc: 0.7965 - val_loss: 1.3782 - val_acc: 0.5136\n",
      "Epoch 92/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.4417 - acc: 0.8132 - val_loss: 1.4669 - val_acc: 0.5776\n",
      "Epoch 93/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.4136 - acc: 0.8309 - val_loss: 1.2990 - val_acc: 0.5824\n",
      "Epoch 94/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.4111 - acc: 0.8230 - val_loss: 1.4954 - val_acc: 0.5696\n",
      "Epoch 95/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.3705 - acc: 0.8520 - val_loss: 1.4627 - val_acc: 0.6064\n",
      "Epoch 96/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.3781 - acc: 0.8407 - val_loss: 1.5116 - val_acc: 0.5744\n",
      "Epoch 97/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.3778 - acc: 0.8427 - val_loss: 1.4805 - val_acc: 0.5872\n",
      "Epoch 98/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.3844 - acc: 0.8397 - val_loss: 1.7776 - val_acc: 0.5088\n",
      "Epoch 99/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.3485 - acc: 0.8574 - val_loss: 1.5237 - val_acc: 0.5584\n",
      "Epoch 100/100\n",
      "2034/2034 [==============================] - 27s - loss: 0.3225 - acc: 0.8732 - val_loss: 1.7675 - val_acc: 0.5072\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(xtrain_sp, ytrain_sp, \n",
    "                 batch_size=100, nb_epoch=100, verbose=1, shuffle = True, \n",
    "                 validation_data=(xtest_sp, ytest_sp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ses02M_script03_1_M015'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2[1800]['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
